{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lstm_testing.ipynb",
      "provenance": [],
      "mount_file_id": "1QFAARcS9OfYid4jrj0zepN4cRVidUlku",
      "authorship_tag": "ABX9TyOfwkZ/fxVOe9cVYEpcbydY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wylhtydtm/Nematode-project/blob/master/cnn_lstm_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFFEzx5cH6Rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import tables\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WqOijb5YIRkG",
        "colab": {}
      },
      "source": [
        "class timeseries_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, hdf5_filename, which_set='train', transform=None):\n",
        "\n",
        "        self.fname = hdf5_filename\n",
        "        self.set_name = which_set\n",
        "        # get labels info\n",
        "        with tables.File(self.fname, 'r') as fid:\n",
        "            tmp = pd.DataFrame.from_records(\n",
        "                fid.get_node('/'+self.set_name)['labels'].read())\n",
        "        self.label_info = tmp[['imaging_plate_drug_concentration', 'MOA_group', 'ts_id']]\n",
        "        # any transform?\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        # I could just use index because ts_id is the same as the index of label_info, but just in case of shuffling...\n",
        "        label_info = self.label_info.iloc[index]\n",
        "        ts_id = label_info['ts_id'].astype(int)\n",
        "        # read data from disk\n",
        "        with tables.File(self.fname, 'r') as fid:\n",
        "          timeseries_data = fid.get_node(\n",
        "                '/' + self.set_name + '/tw_data')[ts_id,:,:].copy()\n",
        "\n",
        "        if self.transform:  # if any transforms were given to initialiser\n",
        "            ts = timeseries_data.astype(np.float32)\n",
        "            #ts = ts.T\n",
        "            ts = self.transform(ts)\n",
        "            ts = ts.squeeze(0)\n",
        "          \n",
        "        # read labels too\n",
        "        labels = label_info['MOA_group']\n",
        "        labels = np.array(labels, dtype=np.float32).reshape(-1, 1)\n",
        "        labels = torch.from_numpy(labels)\n",
        "\n",
        "        return ts, labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af_swA6KIutt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " hd = Path('/content/drive/My Drive')\n",
        " fname = hd / 'Timeseries_testnewsignals__fillednanswifnegative1.hdf'\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jwxEckOIy77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tw_transform= transforms.ToTensor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve8NjAjII0hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = timeseries_dataset(fname, which_set='train',transform=tw_transform)\n",
        "val_data = timeseries_dataset(fname, which_set='val',transform=tw_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaMBXI_zI3BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gVyGYpTv3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a39494c-d71a-49c1-c5d7-a6872e030fd0"
      },
      "source": [
        "print(i1.shape)\n",
        "\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 876, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilVQFgs3t0mm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b3f25a9-29cc-4143-ead7-c74202ed1058"
      },
      "source": [
        "i1.view(i1.shape[0], -1).shape\n",
        "type(i1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Gw2BrWThvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 876\n",
        "hidden_dim = 100 # number of the hidden state \n",
        "num_layers =1  # or 2\n",
        "output_dim =12\n",
        "learning_rate = 0.1\n",
        "\n",
        "#input dimenson:\n",
        "per_element = False\n",
        "if per_element:\n",
        "    lstm_input_size=1\n",
        "else:\n",
        "    lstm_input_size= input_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB83DBdKJuqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim,num_layers,output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        #define the LSTM layer and the output layer\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers,batch_first=True)\n",
        "        # the pytroch output when batch_first= True; ( batch_size, sequence len,, number_directions * hidden_size(feature_dim))\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #Initialise our hidden state and cell state with zeros\n",
        "        h0= torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        \n",
        "        #876 time steps, We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "              \n",
        "        # only take output from the final timestep\n",
        "        out = self.linear(out[:, -1, :]) \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvVAmQcQT5RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM(input_dim, hidden_dim, num_layers=num_layers,output_dim=output_dim)    \n",
        "\n",
        "loss_fn = torch. nn.CrossEntropyLoss() # if we do regression use MSE instead, classification, cross entropy loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np7ig3SSSzR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "964fa04b-fedb-40ed-dce7-f34686529dce"
      },
      "source": [
        "print(model)\n",
        "print(len(list(model.parameters())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(876, 100, batch_first=True)\n",
            "  (linear): Linear(in_features=100, out_features=12, bias=True)\n",
            ")\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPRiV5awc1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7915846e-4b16-4587-947f-8bd7bef4906f"
      },
      "source": [
        "for i in range(len(list(model.parameters()))):\n",
        "    print(list(model.parameters())[i].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 876])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400])\n",
            "torch.Size([400])\n",
            "torch.Size([12, 100])\n",
            "torch.Size([12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvY_j3ZQVMv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_dim = 8  \n",
        "n_iters = 3000\n",
        "iter = 0\n",
        "for epoch in range(100):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as a torch tensor with gradient accumulation abilities\n",
        "        images = images.view(-1, seq_dim, input_size).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        labels = labels.view(-1) # flatten\n",
        "        labels = labels.long()\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in val_loader:\n",
        "                # Resize images\n",
        "                images = images.view(-1, seq_dim, input_dim)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                labels = labels.view(-1) # flatten\n",
        "                labels = labels.long()\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            accuracy = 100 * np.true_divide(correct,total)\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}